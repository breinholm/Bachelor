{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import diffprivlib as dp\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import unidecode\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleaningPipeline(texts):\n",
    "    for i, text in enumerate(texts):\n",
    "        text = text.lower()\n",
    "        text = unidecode.unidecode(text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        re.sub(\"\\s\\s+\" , \" \", text)\n",
    "        texts[i] = text\n",
    "    return texts\n",
    "def loadAndCleanDatasetBalanced(datasetName, _language):\n",
    "    dataset = load_dataset(datasetName, language=_language)\n",
    "    x_train = dataset[\"train\"][\"text\"]\n",
    "    x_train = CleaningPipeline(x_train)\n",
    "    y_train = dataset[\"train\"][\"label\"]\n",
    "    \n",
    "    positiveIndices = [i for i, x in enumerate(y_train) if x == 1]\n",
    "    random.shuffle(positiveIndices)\n",
    "    maxValues = min(5000,len(positiveIndices))\n",
    "    posIdxs = positiveIndices[0:maxValues]\n",
    "    \n",
    "    negativeIndices = [i for i, x in enumerate(y_train) if x == 0]\n",
    "    random.shuffle(negativeIndices)\n",
    "    negIdxs = negativeIndices[0:maxValues]\n",
    "    canton_train = dataset[\"train\"][\"canton\"]\n",
    "    region_train = dataset[\"train\"][\"region\"]\n",
    "    \n",
    "    x_train_bal = []\n",
    "    y_train_bal = []\n",
    "    canton_train_bal = []\n",
    "    region_train_bal = []\n",
    "    for posIdx in posIdxs:\n",
    "        x_train_bal.append(x_train[posIdx])\n",
    "        y_train_bal.append(y_train[posIdx])\n",
    "        canton_train_bal.append(canton_train[posIdx])\n",
    "        region_train_bal.append(region_train[posIdx])\n",
    "    for negIdx in negIdxs:\n",
    "        x_train_bal.append(x_train[negIdx])\n",
    "        y_train_bal.append(y_train[negIdx])\n",
    "        canton_train_bal.append(canton_train[negIdx])\n",
    "        region_train_bal.append(region_train[negIdx])\n",
    "    \n",
    "    x_val = dataset[\"validation\"][\"text\"]\n",
    "    x_val  = CleaningPipeline(x_val)\n",
    "    y_val  = dataset[\"validation\"][\"label\"]\n",
    "    canton_val = dataset[\"validation\"][\"canton\"]\n",
    "    region_val = dataset[\"validation\"][\"region\"]\n",
    "\n",
    "    x_test = dataset[\"test\"][\"text\"]\n",
    "    x_test = CleaningPipeline(x_test)\n",
    "    y_test = dataset[\"test\"][\"label\"]\n",
    "    canton_test = dataset[\"test\"][\"canton\"]\n",
    "    region_test = dataset[\"test\"][\"region\"]\n",
    "    print(len(y_train_bal))\n",
    "    return x_train_bal, y_train_bal, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train_bal, region_train_bal\n",
    "def loadAndCleanDatasetImbalanced(datasetName, _language):\n",
    "    dataset = load_dataset(datasetName, language=_language)\n",
    "    x_train = dataset[\"train\"][\"text\"][0:10000]\n",
    "    x_train = CleaningPipeline(x_train)\n",
    "    y_train = dataset[\"train\"][\"label\"][0:10000]\n",
    "    \n",
    "    x_val = dataset[\"validation\"][\"text\"]\n",
    "    x_val  = CleaningPipeline(x_val)[0:10000]\n",
    "    y_val  = dataset[\"validation\"][\"label\"][0:10000]\n",
    "    canton_val = dataset[\"validation\"][\"canton\"]\n",
    "    region_val = dataset[\"validation\"][\"region\"]\n",
    "    \n",
    "    x_test = dataset[\"test\"][\"text\"]\n",
    "    x_test = CleaningPipeline(x_test)\n",
    "    y_test = dataset[\"test\"][\"label\"]\n",
    "    canton_test = dataset[\"test\"][\"canton\"]\n",
    "    canton_train = dataset[\"train\"][\"canton\"][0:10000]\n",
    "    region_test = dataset[\"test\"][\"region\"]\n",
    "    region_train = dataset[\"train\"][\"region\"][0:10000]\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train\n",
    "def Multiply(x, w):\n",
    "    return np.multiply(x,w)\n",
    "    \n",
    "\n",
    "def VisualizeDatasetSklearn(x_val, canton_val, region_val, coef, name):\n",
    "    print(x_val.shape)\n",
    "    print(coef.shape)\n",
    "\n",
    "    coef = coef.astype(np.float16)\n",
    "    dataToVisualize = Multiply(x_val,coef)\n",
    "    print(dataToVisualize)\n",
    "    tsne = TSNEVisualizer(colormap = 'tab20c', title = 'TSNE with colored cantons, model: '+name)\n",
    "    tsne.fit(dataToVisualize, canton_val)\n",
    "    tsne.show()\n",
    "    tsne = TSNEVisualizer(colormap = 'tab20c', title = 'TSNE with colored regions, model: '+name)\n",
    "    tsne.fit(dataToVisualize, region_val)\n",
    "    tsne.show()\n",
    "    \n",
    "def VisualizeDatasetSklearn2(x_val, canton_val, region_val, coef, name):\n",
    "    coef = coef.astype(np.float16)\n",
    "    dataToVisualize = Multiply(x_val.toarray(),coef)\n",
    "    tsne = TSNEVisualizer(colormap = 'tab20c', title = 'TSNE with colored cantons, model: '+name)\n",
    "    tsne.fit(dataToVisualize, canton_val)\n",
    "    tsne.show()\n",
    "    tsne = TSNEVisualizer(colormap = 'tab20c', title = 'TSNE with colored regions, model: '+name)\n",
    "    tsne.fit(dataToVisualize, region_val)\n",
    "    tsne.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStatistics():\n",
    "    cantonDict = {}\n",
    "    regionDict = {}\n",
    "    for language in languages:\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train = loadAndCleanDataset('fscs',language)\n",
    "        for i, canton in enumerate(canton_train):\n",
    "            if canton in cantonDict:\n",
    "                cantonDict[canton][language][y_train[i]] += 1\n",
    "            else:\n",
    "                cantonDict[canton] = {'de': [0, 0], 'fr': [0, 0],'it': [0, 0]}\n",
    "                cantonDict[canton][language][y_train[i]] +=1\n",
    "        for i, canton in enumerate(canton_test):\n",
    "            if canton in cantonDict:\n",
    "                 cantonDict[canton][language][y_test[i]] += 1\n",
    "            else:\n",
    "                cantonDict[canton] = {'de': [0, 0], 'fr': [0, 0],'it': [0, 0]}\n",
    "                cantonDict[canton][language][y_test[i]] +=1\n",
    "         \n",
    "        for i, region in enumerate(region_train):\n",
    "            if region in regionDict:\n",
    "                regionDict[region][language][y_train[i]] += 1\n",
    "            else:\n",
    "                regionDict[region] = {'de': [0, 0], 'fr': [0, 0],'it': [0, 0]}\n",
    "                regionDict[region][language][y_train[i]] +=1\n",
    "        for i, region in enumerate(region_test):\n",
    "            if region in regionDict:\n",
    "                regionDict[region][language][y_test[i]] += 1\n",
    "            else:\n",
    "                regionDict[region] = {'de': [0, 0], 'fr': [0, 0],'it': [0, 0]}\n",
    "                regionDict[region][language][y_test[i]] +=1\n",
    "                \n",
    "    file1 = open(\"canton.txt\",\"a\")\n",
    "    for canton, languageDict in cantonDict.items():\n",
    "        file1.write(str(canton)+\";\")\n",
    "        for language2, value in languageDict.items():\n",
    "            file1.write(str(value[0])+\";\"+str(value[1])+\";\")\n",
    "        file1.write(\"\\n\")\n",
    "\n",
    "    file1.close()\n",
    "            \n",
    "    file1 = open(\"region.txt\",\"a\")\n",
    "    for region, languageDict in regionDict.items():\n",
    "        file1.write(str(region)+\";\")\n",
    "        for language2, value in languageDict.items():\n",
    "            file1.write(str(value[0])+\";\"+str(value[1])+\";\")\n",
    "        file1.write(\"\\n\")\n",
    "    file1.close()\n",
    "\n",
    "    return cantonDict, regionDict\n",
    "\n",
    "\n",
    "def getTotalDist(language, canton_train, canton_test, region_train, region_test):\n",
    "        cantonDict = {}\n",
    "        regionDict = {}\n",
    "        for canton in canton_test:\n",
    "            if canton in cantonDict:\n",
    "                cantonDict[canton] += 1\n",
    "            else:\n",
    "                cantonDict[canton] = 1\n",
    "        for canton in canton_train:\n",
    "            if canton in cantonDict:\n",
    "                cantonDict[canton] += 1\n",
    "            else:\n",
    "                cantonDict[canton] = 1\n",
    "        for region in region_train:\n",
    "            if region in regionDict:\n",
    "                regionDict[region] += 1\n",
    "            else:\n",
    "                regionDict[region] = 1\n",
    "        for region in region_test:\n",
    "            if region in regionDict:\n",
    "                regionDict[region] += 1\n",
    "            else:\n",
    "                regionDict[region] = 1\n",
    "        return cantonDict, regionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def CalculateOddsAndRiskFixed(predictions, labels, sensitive_col, sensitiveDict):\n",
    "    accuracies = []\n",
    "    odds = []\n",
    "    values = set(sensitive_col)\n",
    "    for sensitive_value in values:\n",
    "        if sensitiveDict[sensitive_value] < 5: #Filtering mechanism\n",
    "            continue\n",
    "        positives = 0\n",
    "        count = 0\n",
    "        accuracy = 0\n",
    "        for i, value in enumerate(sensitive_col):\n",
    "            if sensitive_value == value:\n",
    "                count += 1\n",
    "                if predictions[i] == 1:\n",
    "                    positives += 1\n",
    "                if predictions[i] == labels[i]:\n",
    "                    accuracy += 1\n",
    "        accuracies.append(accuracy/count)\n",
    "        odds.append(positives/count)\n",
    "    return accuracies, odds   \n",
    "def CalculatePositiveAccuracy(predictions, labels):\n",
    "    P = 0\n",
    "    TP = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            P += 1\n",
    "            if label == predictions[i]:\n",
    "                TP+=1\n",
    "    return TP/P\n",
    "def EvaluateResults(predictions, labels, sensitive_attributes, canton_test, region_test, language,  cantonDict, regionDict):\n",
    "    equal_risk = []\n",
    "    equal_odds = []\n",
    "    micro = f1_score(labels,predictions, average='micro')\n",
    "    macro = f1_score(labels,predictions, average='macro')\n",
    "  \n",
    "    positive_accuracy = CalculatePositiveAccuracy(predictions,labels)\n",
    "    canton_acc, canton_odds = CalculateOddsAndRiskFixed(predictions,labels,canton_test,cantonDict)\n",
    "    region_acc, region_odds = CalculateOddsAndRiskFixed(predictions,labels,region_test,regionDict)\n",
    "        \n",
    "\n",
    "    equal_risk.append((sensitive_attributes[0][0], max(canton_acc)-min(canton_acc)))\n",
    "    equal_risk_var_canton = np.var(canton_acc)\n",
    "    equal_odds.append((sensitive_attributes[0][0], max(canton_odds)-min(canton_odds)))\n",
    "    equal_odds_var_canton = np.var(canton_odds)\n",
    "    equal_risk.append((sensitive_attributes[1][0], max(region_acc)-min(region_acc)))\n",
    "    equal_risk_var_region = np.var(region_acc)\n",
    "    equal_odds.append((sensitive_attributes[1][0], max(region_odds)-min(region_odds)))\n",
    "    equal_odds_var_region = np.var(region_odds)\n",
    "    return micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSklearnModel(vectorizer, privacy_on, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon):\n",
    "    x_train = vectorizer.fit_transform(x_train)\n",
    "    x_val = vectorizer.transform(x_val)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    cantonDict, regionDict = getTotalDist(language, canton_train, canton_test, region_train, region_test)\n",
    "    #language,model,vectorizer, f1_micro, f1_macro, equal_risk_canton, equal_risk_region, equal_odds_canton, equal_risk_region\n",
    "    file1 = open(\"resultsWithBalancedDatasetEpsilonnew\"+str(epsilon)+\".txt\",\"a\")\n",
    "    if privacy_on:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            model = dp.models.LogisticRegression(epsilon = epsilon)\n",
    "            model.fit(x_train.A,y_train)\n",
    "            y_pred = model.predict(x_test)   \n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + str(\"logisticregression_dp_\")+str(vectorizer) + \" BalancedDataSet\"\n",
    "                VisualizeDatasetSklearn2(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language + \";\" + str(\"logisticregression_dp\")+\";\"+str(vectorizer)+ \";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")\n",
    "    else:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            print(k)\n",
    "            model = LogisticRegression()\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred = model.predict(x_test)   \n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + \"logisticregression_\"+str(vectorizer) + \" BalancedDataSet\"\n",
    "                VisualizeDatasetSklearn2(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language + \";\" + str(model)+\";\"+str(vectorizer)+ \";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")    \n",
    "            #filename = language + \",\" + str(model)+\",\"+str(vectorizer)\n",
    "                #VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename) UNCOMMENT TO VISUALIZE\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortenTokenization(input_ids, attention_mask):\n",
    "    if(input_ids.shape[1] > 512):\n",
    "        input_ids = input_ids[0,:512].reshape((1,512))\n",
    "        attention_mask = attention_mask[0,:512].reshape((1,512))\n",
    "        result = dict([(\"input_ids\", input_ids.cuda()), (\"attention_mask\",attention_mask.cuda())])\n",
    "    result = dict([(\"input_ids\", input_ids.cuda()), (\"attention_mask\", attention_mask.cuda())])\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def RobertaTokenizeAndTransformData(data, tokenizer, transformModel, mt5):\n",
    "    transformedData = []\n",
    "    x_train = np.zeros((len(data),768)) #subject to change depending on which model is run\n",
    "    for i, dataSample in enumerate(data):\n",
    "        tokenizedDataSample = tokenizer(dataSample, return_tensors='pt')\n",
    "        input_ids = tokenizedDataSample['input_ids']\n",
    "        attention_mask = tokenizedDataSample['attention_mask']\n",
    "        tokenizedDataSample = shortenTokenization(input_ids, attention_mask)\n",
    "        if(mt5):\n",
    "            transformedDataSample = transformModel(input_ids=tokenizedDataSample[\"input_ids\"], decoder_input_ids=tokenizedDataSample[\"input_ids\"]).last_hidden_state.cpu().detach().numpy()\n",
    "            x_train[i,:] = np.mean(transformedDataSample[0,:,:], axis=0)\n",
    "        else:\n",
    "            transformedDataSample = transformModel(**tokenizedDataSample).pooler_output.detach().cpu().numpy()\n",
    "            x_train[i,:] = transformedDataSample[0,:]\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMt5Model(naming, privacy_on, tokenizer, transformModel, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon):\n",
    "    x_train = RobertaTokenizeAndTransformData(x_train, tokenizer, transformModel, True)\n",
    "    x_test = RobertaTokenizeAndTransformData(x_test,tokenizer,transformModel, True)\n",
    "    x_val = RobertaTokenizeAndTransformData(x_val, tokenizer, transformModel, True)\n",
    "    cantonDict, regionDict = getTotalDist(language, canton_train, canton_test, region_train, region_test)\n",
    "    #language,model,vectorizer, f1_micro, f1_macro, equal_risk_canton, equal_risk_region, equal_odds_canton, equal_risk_region\n",
    "    file1 = open(\"resultsWithBalancedDatasetEpsilonnew\"+str(epsilon)+\".txt\",\"a\")\n",
    "    if privacy_on:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            model = dp.models.LogisticRegression(epsilon = epsilon)\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred = model.predict(x_test)   \n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + \"logisticregression_dp\" + naming\n",
    "                VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language  +\";\" + \"logisticregression_dp_\" + naming +\";\"+naming+\"Vectorizer\"+\";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")\n",
    "        #VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "      \n",
    "    else:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            model = LogisticRegression()\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred = model.predict(x_test)   \n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + \"logisticregression\" + naming +\";\"+naming+\"Vectorizer\"\n",
    "                VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language + \";\" + \"logisticregression_\" + naming +\";\"+naming+\"Vectorizer\"+\";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")\n",
    "        #VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        #outfile = open(filename,'wb')\n",
    "        #pickle.dump(y_pred, outfile)\n",
    "        #outfile.close()\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRobertaModel(naming, privacy_on, tokenizer, transformModel, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon):\n",
    "    x_train = RobertaTokenizeAndTransformData(x_train, tokenizer, transformModel, False)\n",
    "    x_test = RobertaTokenizeAndTransformData(x_test,tokenizer,transformModel, False)\n",
    "    x_val = RobertaTokenizeAndTransformData(x_val, tokenizer, transformModel, False)\n",
    "    cantonDict, regionDict = getTotalDist(language, canton_train, canton_test, region_train, region_test)\n",
    "\n",
    "    #language,model,vectorizer, f1_micro, f1_macro, equal_risk_canton, equal_risk_region, equal_odds_canton, equal_risk_region\n",
    "    file1 = open(\"resultsWithBalancedDatasetEpsilonnew\"+str(epsilon)+\".txt\",\"a\")\n",
    "    if privacy_on:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            model = dp.models.LogisticRegression(epsilon = epsilon)\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            \n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + \"logisticregression_dp_\" + naming\n",
    "                VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language + \";\" + \"logisticregression_dp_\" + naming +\";\"+naming+\"Vectorizer\"+\";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")\n",
    "    else:\n",
    "        totalMicro = 0\n",
    "        totalMacro = 0\n",
    "        totalRiskCanton = 0\n",
    "        totalRiskRegion = 0\n",
    "        totalOddsCanton = 0\n",
    "        totalOddsRegion = 0\n",
    "        totalOdds = 0\n",
    "        totalPosAcc = 0\n",
    "        equalVarRiskCanton = 0\n",
    "        equalVarOddsCanton = 0\n",
    "        equalVarRiskRegion = 0\n",
    "        equalVarOddsRegion = 0\n",
    "        for k in range(5): #set the 5 to 1, and avoid dividing by 5 in the result-writer to get the non-averaged results\n",
    "            model = LogisticRegression()\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "            totalMicro += micro\n",
    "            totalMacro += macro\n",
    "            totalPosAcc += positive_accuracy\n",
    "            totalRiskCanton += equal_risk[0][1]\n",
    "            totalRiskRegion += equal_risk[1][1]\n",
    "            totalOddsCanton += equal_odds[0][1]\n",
    "            totalOddsRegion += equal_odds[1][1]\n",
    "            equalVarRiskCanton += equal_risk_var_canton\n",
    "            equalVarOddsCanton += equal_odds_var_canton\n",
    "            equalVarRiskRegion += equal_risk_var_region\n",
    "            equalVarOddsRegion += equal_odds_var_region\n",
    "            if k % 3 == 0:\n",
    "                filename = language + \";\" + \"logisticregression_\" + naming\n",
    "                VisualizeDatasetSklearn(x_val, canton_val, region_val, model.coef_, filename)\n",
    "        file1.write(language + \";\" + \"logisticregression_\" + naming +\";\"+naming+\"Vectorizer\"+\";\"+str(totalMicro/5) +\";\"+ str(totalMacro/5) + \";\" + str(totalPosAcc/5) + \";\" + str(totalRiskCanton/5) + \";\" + str(totalRiskRegion/5) + \";\"+str(equalVarRiskCanton/5) + \";\"+str(equalVarRiskRegion/5) +\";\" + str(totalOddsCanton/5) + \";\" + str(totalOddsRegion/5)+ \";\" + str(equalVarOddsCanton/5) + \";\" + str(equalVarOddsRegion/5) +\"\\n\")\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-potter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import MT5Model, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import diffprivlib as dp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "languages = ['de','fr','it']\n",
    "mt5_tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "mt5_model = MT5Model.from_pretrained(\"google/mt5-base\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\").cuda()\n",
    "epsilons =[1,5]\n",
    "for language in languages:\n",
    "    for epsilon in epsilons:\n",
    "        #uncomment for imbalanced dataset\n",
    "    #x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train = loadAndCleanDatasetImbalanced('fscs',language)\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train = loadAndCleanDatasetBalanced('fscs',language)\n",
    "        countVectorizer = CountVectorizer()\n",
    "        testSklearnModel(countVectorizer, False,x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "        testSklearnModel(countVectorizer, True,x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "        tfidfVectorizer = TfidfVectorizer()\n",
    "        testSklearnModel(tfidfVectorizer, False,x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "        testSklearnModel(tfidfVectorizer, True,x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "   \n",
    "        testMt5Model(\"mt5-base\", False,  mt5_tokenizer, mt5_model, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "        testMt5Model(\"mt5-base\", True,  mt5_tokenizer, mt5_model, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "    \n",
    "        testRobertaModel(\"xlm-roberta-base\", False, tokenizer, model, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "        testRobertaModel(\"xlm-roberta-base\",True, tokenizer, model, x_train, y_train, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train, region_train, epsilon)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def testMajorityBaseLine():\n",
    "    languages = ['de','fr','it']\n",
    "    for language in languages:\n",
    "        x_train_bal, y_train_bal, x_val, y_val, x_test, y_test, canton_test, region_test, canton_val, region_val, canton_train_bal, region_train_bal = loadAndCleanDatasetBalanced('fscs',language)\n",
    "        y_pred = [0] *len(y_test)\n",
    "        cantonDict, regionDict = getTotalDist(language, canton_train, canton_test, region_train, region_test)\n",
    "       # y_pred = model.predict(x_test)\n",
    "        micro, macro, equal_risk, equal_odds, positive_accuracy,equal_risk_var_canton,equal_odds_var_canton,equal_risk_var_region,equal_odds_var_region = EvaluateResults(y_pred, y_test, [(\"canton\", canton_test),(\"region\", region_test)], canton_test, region_test, language, cantonDict, regionDict)\n",
    "        #language,model,vectorizer, f1_micro, f1_macro, equal_risk_canton, equal_risk_region, equal_odds_canton, equal_risk_region\n",
    "        file1 = open(\"results.txt\",\"a\")\n",
    "        file1.write(language + \",\" + \"majorityBaseLine\"+\",\"+\"no_vectorizer\"+\",\"+ str(micro) +\",\"+ str(macro) +\",\" + str(equal_risk[0][1]) + \",\" + str(equal_risk[1][1]) + \",\" + str(equal_odds[0][1]) + \",\" + str(equal_odds[1][1])+ \"\\n\")\n",
    "        file1.close()\n",
    "        \n",
    "testMajorityBaseLine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-message",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-cherry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-toolbox",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-copper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-force",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-dress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-feeding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
